{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# **Node Property Prediction via Graph Convolutional Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gzsP50bF6Gb"
      },
      "source": [
        "In this tutorial, we will construct our own graph neural network by using PyTorch Geometric (PyG) and apply the model on a Open Graph Benchmark (OGB) dataset. The dataset is used to benchmark the model performance on node property prediction, predicting properties of single nodes.\n",
        "\n",
        "At first, we will learn how PyTorch Geometric stores the graphs in PyTorch tensor.\n",
        "\n",
        "We will then load and take a quick look on one of the Open Graph Benchmark (OGB) datasets by using the `ogb` package. OGB is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. The `ogb` package not only provides the data loader of the dataset but also the evaluator.\n",
        "\n",
        "At last, we will build our own graph neural networks by using PyTorch Geometric. And then apply and evaluate the models on the node property prediction task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0NiFL6OLpaJ"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgHmEvVPlWUW"
      },
      "source": [
        "Please make sure to install the following packages by running the following codes before you officially start the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "By2oyBw7Lrh5",
        "outputId": "6295488a-77e9-45cf-cba9-618659593433"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.0+cu113\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 4.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 4.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 407 kB 4.8 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.3-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.64.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.12.0+cu113)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2022.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=41379208c5c1e244a140eefd968b49058d6f641609f32241c41f08ea5a91d1a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.3 outdated-0.2.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.12.0+cu113.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.12.0+cu113.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install ogb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nwwq0nSdmsOL"
      },
      "source": [
        "# Exercise 1: PyTorch Geometric\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf7vUmdNKCjA"
      },
      "source": [
        "PyTorch Geometric generally has two classes for storing or transforming the graphs into tensor format. One is the `torch_geometric.datasets`, which contains a variety of common graph datasets. Another one is `torch_geometric.data` that provides the data handling of graphs in PyTorch tensors.\n",
        "\n",
        "In this section, we will learn how to use the `torch_geometric.datasets` and `torch_geometric.data`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic-o1P3r6hr2"
      },
      "source": [
        "## PyG Datasets\n",
        "\n",
        "The `torch_geometric.datasets` has many common graph datasets. Here we will explore the usage by using one example dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT5qca3x6XpG",
        "outputId": "331cd256-4ebd-4b85-da9f-7be26690e416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip\n",
            "Extracting enzymes/ENZYMES/ENZYMES.zip\n",
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENZYMES(600)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "\n",
        "root = './enzymes'\n",
        "name = 'ENZYMES'\n",
        "\n",
        "# The ENZYMES dataset\n",
        "pyg_dataset= TUDataset('./enzymes', 'ENZYMES')\n",
        "\n",
        "# You can find that there are 600 graphs in this dataset\n",
        "print(pyg_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLm5vVYMAP2x"
      },
      "source": [
        "## Question 1: What is the number of classes and number of features in the ENZYMES dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iF_Kyqr_JbY",
        "outputId": "9ed0c8c3-9d20-4995-8681-85c2b29b0d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENZYMES dataset has 0 classes\n",
            "ENZYMES dataset has 0 features\n"
          ]
        }
      ],
      "source": [
        "def get_num_classes(pyg_dataset):\n",
        "  # TODO: Implement this function that takes a PyG dataset object\n",
        "  # and return the number of classes for that dataset.\n",
        "\n",
        "  num_classes = 0\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~1 line of code)\n",
        "  ## Note\n",
        "  ## 1. Colab autocomplete functionality might be useful.\n",
        "  #########################################\n",
        "\n",
        "  return num_classes\n",
        "\n",
        "def get_num_features(pyg_dataset):\n",
        "  # TODO: Implement this function that takes a PyG dataset object\n",
        "  # and return the number of features for that dataset.\n",
        "\n",
        "  num_features = 0\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~1 line of code)\n",
        "  ## Note\n",
        "  ## 1. Colab autocomplete functionality might be useful.\n",
        "  #########################################\n",
        "\n",
        "  return num_features\n",
        "\n",
        "# You may find that some information need to be stored in the dataset level,\n",
        "# specifically if there are multiple graphs in the dataset\n",
        "\n",
        "num_classes = get_num_classes(pyg_dataset)\n",
        "num_features = get_num_features(pyg_dataset)\n",
        "print(\"{} dataset has {} classes\".format(name, num_classes))\n",
        "print(\"{} dataset has {} features\".format(name, num_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwKbzhHUAckZ"
      },
      "source": [
        "## PyG Data\n",
        "\n",
        "Each PyG dataset usually stores a list of `torch_geometric.data.Data` objects. Each `torch_geometric.data.Data` object usually represents a graph. You can easily get the `Data` object by indexing on the dataset.\n",
        "\n",
        "For more information such as what will be stored in `Data` object, please refer to the [documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sCV3xJWCddX"
      },
      "source": [
        "## Question 2: What is the label of the graph (index 100 in the ENZYMES dataset)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIis9oTZAfs3",
        "outputId": "17f5cd17-30c5-4f48-bf53-a25704a8eeb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(edge_index=[2, 168], x=[37, 3], y=[1])\n",
            "Graph with index 100 has label -1\n"
          ]
        }
      ],
      "source": [
        "def get_graph_class(pyg_dataset, idx):\n",
        "  # TODO: Implement this function that takes a PyG dataset object,\n",
        "  # the index of the graph in dataset, and returns the class/label \n",
        "  # of the graph (in integer).\n",
        "\n",
        "  label = -1\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~1 line of code)\n",
        "  #########################################\n",
        "\n",
        "  return label\n",
        "\n",
        "# Here pyg_dataset is a dataset for graph classification\n",
        "graph_0 = pyg_dataset[0]\n",
        "print(graph_0)\n",
        "idx = 100\n",
        "label = get_graph_class(pyg_dataset, idx)\n",
        "print('Graph with index {} has label {}'.format(idx, label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXa7yIG4E0Fp"
      },
      "source": [
        "# Exercise 2: Open Graph Benchmark (OGB)\n",
        "\n",
        "The Open Graph Benchmark (OGB) is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. Its datasets are automatically downloaded, processed, and split using the OGB Data Loader. The model performance can also be evaluated by using the OGB Evaluator in a unified manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnazPGGAJAZN"
      },
      "source": [
        "## Dataset and Data\n",
        "\n",
        "OGB also supports the PyG dataset and data. Here we take a look on the `ogbn-arxiv` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gpc6bTm3GF02",
        "outputId": "cf7625cc-1fa6-479e-9a0c-10e4687afc2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:02<00:00, 33.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset/arxiv.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 8738.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 4112.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ogbn-arxiv dataset has 1 graph\n",
            "Data(num_nodes=169343, x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243])\n"
          ]
        }
      ],
      "source": [
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "\n",
        "dataset_name = 'ogbn-arxiv'\n",
        "# Load the dataset and transform it to sparse tensor\n",
        "dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                 transform=T.ToSparseTensor())\n",
        "print('The {} dataset has {} graph'.format(dataset_name, len(dataset)))\n",
        "\n",
        "# Extract the graph\n",
        "data = dataset[0]\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw0xZJKZI-n3"
      },
      "source": [
        "## Question 3: What is the number of features in the ogbn-arxiv graph?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP844_nT2ZJl",
        "outputId": "4c38216c-13f1-4605-edfe-8b9a8a70b50c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The graph has 0 features\n"
          ]
        }
      ],
      "source": [
        "def graph_num_features(data):\n",
        "  # TODO: Implement this function that takes a PyG data object,\n",
        "  # and returns the number of features in the graph (in integer).\n",
        "\n",
        "  num_features = 0\n",
        "\n",
        "  ############# Your code here ############\n",
        "  ## (~1 line of code)\n",
        "  #########################################\n",
        "\n",
        "  return num_features\n",
        "\n",
        "num_features = graph_num_features(data)\n",
        "print('The graph has {} features'.format(num_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DP_yEQZ0NVW"
      },
      "source": [
        "# Exercise 3: Node Property Prediction via GCN\n",
        "\n",
        "In this section we will build our first graph neural network by using PyTorch Geometric and apply it on node property prediction (node classification).\n",
        "\n",
        "We will build the graph neural network by using GCN operator ([Kipf et al. (2017)](https://arxiv.org/pdf/1609.02907.pdf)).\n",
        "\n",
        "You should use the PyG built-in `GCNConv` layer directly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4CcOUEoInjD"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DCtgcHpGIpd",
        "outputId": "e87e2189-ffcb-46aa-cb4d-9e5d2459eaee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.0+cu113\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)\n",
        "\n",
        "# The PyG built-in GCNConv\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IK9z0wQIwzQ"
      },
      "source": [
        "## Load and Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ibJ0ieoIwQM",
        "outputId": "78a3c563-9903-4df3-b634-ef4701dda3bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "dataset_name = 'ogbn-arxiv'\n",
        "dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                 transform=T.ToSparseTensor())\n",
        "data = dataset[0]\n",
        "\n",
        "# Make the adjacency matrix to symmetric\n",
        "data.adj_t = data.adj_t.to_symmetric()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# If you use GPU, the device should be cuda\n",
        "print('Device: {}'.format(device))\n",
        "\n",
        "data = data.to(device)\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx = split_idx['train'].to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgUA815bNJ8w"
      },
      "source": [
        "## GCN Model\n",
        "\n",
        "Now we will implement our GCN model!\n",
        "\n",
        "Please follow the figure below to implement your `forward` function.\n",
        "\n",
        "\n",
        "![test](https://drive.google.com/uc?id=128AuYAXNXGg7PIhJJ7e420DoPWKb-RtL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IgspXTYpNJLA"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
        "                 dropout, return_embeds=False):\n",
        "        # TODO: Implement this function that initializes self.convs, \n",
        "        # self.bns, and self.softmax.\n",
        "\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        # A list of GCNConv layers\n",
        "        self.convs = None\n",
        "\n",
        "        # A list of 1D batch normalization layers\n",
        "        self.bns = None\n",
        "\n",
        "        # The log softmax layer\n",
        "        self.softmax = None\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## Note:\n",
        "        ## 1. You should use torch.nn.ModuleList for self.convs and self.bns\n",
        "        ## 2. self.convs has num_layers GCNConv layers\n",
        "        ## 3. self.bns has num_layers - 1 BatchNorm1d layers\n",
        "        ## 4. You should use torch.nn.LogSoftmax for self.softmax\n",
        "        ## 5. The parameters you can set for GCNConv include 'in_channels' and \n",
        "        ## 'out_channels'. More information please refer to the documentation:\n",
        "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv\n",
        "        ## 6. The only parameter you need to set for BatchNorm1d is 'num_features'\n",
        "        ## More information please refer to the documentation: \n",
        "        ## https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
        "        ## (~10 lines of code)\n",
        "\n",
        "        #########################################\n",
        "\n",
        "        # Probability of an element to be zeroed\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Skip classification layer and return node embeddings\n",
        "        self.return_embeds = return_embeds\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        # TODO: Implement this function that takes the feature tensor x,\n",
        "        # edge_index tensor adj_t and returns the output tensor as\n",
        "        # shown in the figure.\n",
        "\n",
        "        out = None\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## Note:\n",
        "        ## 1. Construct the network as showing in the figure\n",
        "        ## 2. torch.nn.functional.relu and torch.nn.functional.dropout are useful\n",
        "        ## More information please refer to the documentation:\n",
        "        ## https://pytorch.org/docs/stable/nn.functional.html\n",
        "        ## 3. Don't forget to set F.dropout training to self.training\n",
        "        ## 4. If return_embeds is True, then skip the last softmax layer\n",
        "        ## (~7 lines of code)\n",
        "\n",
        "        #########################################\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FF1hnHUhO81e"
      },
      "outputs": [],
      "source": [
        "def train(model, data, train_idx, optimizer, loss_fn):\n",
        "    # TODO: Implement this function that trains the model by \n",
        "    # using the given optimizer and loss_fn.\n",
        "    model.train()\n",
        "    loss = 0\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## Note:\n",
        "    ## 1. Zero grad the optimizer\n",
        "    ## 2. Feed the data into the model\n",
        "    ## 3. Slicing the model output and label by train_idx\n",
        "    ## 4. Feed the sliced output and label to loss_fn\n",
        "    ## (~4 lines of code)\n",
        "\n",
        "    #########################################\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aJdlrJQhPBsK"
      },
      "outputs": [],
      "source": [
        "# Test function here\n",
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator):\n",
        "    # TODO: Implement this function that tests the model by \n",
        "    # using the given split_idx and evaluator.\n",
        "    model.eval()\n",
        "\n",
        "    # The output of model on all data\n",
        "    out = None\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## (~1 line of code)\n",
        "    ## Note:\n",
        "    ## 1. No index slicing here\n",
        "\n",
        "    #########################################\n",
        "\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    return train_acc, valid_acc, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "o7F46xkuLiOL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c55340b-e14c-4017-dd47-0b793c258b62"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'device': 'cuda',\n",
              " 'dropout': 0.5,\n",
              " 'epochs': 100,\n",
              " 'hidden_dim': 256,\n",
              " 'lr': 0.01,\n",
              " 'num_layers': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Please do not change the args\n",
        "args = {\n",
        "    'device': device,\n",
        "    'num_layers': 3,\n",
        "    'hidden_dim': 256,\n",
        "    'dropout': 0.5,\n",
        "    'lr': 0.01,\n",
        "    'epochs': 100,\n",
        "}\n",
        "args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT8RyM2cPGxM"
      },
      "outputs": [],
      "source": [
        "model = GCN(data.num_features, args['hidden_dim'],\n",
        "            dataset.num_classes, args['num_layers'],\n",
        "            args['dropout']).to(device)\n",
        "evaluator = Evaluator(name='ogbn-arxiv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd5O5cnPPdVF"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "# reset the parameters to initial random value\n",
        "model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "loss_fn = F.nll_loss\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "  loss = train(model, data, train_idx, optimizer, loss_fn)\n",
        "  result = test(model, data, split_idx, evaluator)\n",
        "  train_acc, valid_acc, test_acc = result\n",
        "  if valid_acc > best_valid_acc:\n",
        "      best_valid_acc = valid_acc\n",
        "      best_model = copy.deepcopy(model)\n",
        "  print(f'Epoch: {epoch:02d}, '\n",
        "        f'Loss: {loss:.4f}, '\n",
        "        f'Train: {100 * train_acc:.2f}%, '\n",
        "        f'Valid: {100 * valid_acc:.2f}% '\n",
        "        f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqcextqOL2FX"
      },
      "outputs": [],
      "source": [
        "best_result = test(best_model, data, split_idx, evaluator)\n",
        "train_acc, valid_acc, test_acc = best_result\n",
        "print(f'Best model: '\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * valid_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "tutorial_8_template.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}